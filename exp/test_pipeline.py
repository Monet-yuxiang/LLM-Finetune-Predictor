"""
æµ‹è¯•æ–‡ä»¶ - éªŒè¯å®Œæ•´çš„è®­ç»ƒ-è¯„ä¼°æµæ°´çº¿
ä¸»è¦åŠŸèƒ½ï¼š
1. æµ‹è¯•å•ä¸ªæ¨¡å—åŠŸèƒ½
2. æµ‹è¯•å®Œæ•´æµæ°´çº¿
3. éªŒè¯å†…å­˜ç®¡ç†
4. æ£€æŸ¥ç»“æœæ ¼å¼
"""

import os
import json
import sys
import time
import torch
import gc
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥æ¨¡å—
from train_module import train_model_on_dataset_with_progress
from evaluate_module import evaluate_model_on_dataset_with_progress
from pipeline_module import run_pipeline_with_progress

def test_individual_modules():
    """æµ‹è¯•å•ä¸ªæ¨¡å—åŠŸèƒ½"""
    print("=" * 80)
    print("ğŸ§ª æµ‹è¯•å•ä¸ªæ¨¡å—åŠŸèƒ½")
    print("=" * 80)
    
    # æµ‹è¯•é…ç½®
    model_path = "/home/haonan/data_decision/models/Qwen/Qwen2.5-7B-Instruct-1M"
    dataset_path = "/home/haonan/data_decision/example/qwen_dataset/qa_dataset_20250709_1_25.json"
    lora_config_dir = "/home/haonan/data_decision/lora_configs"  # éœ€è¦åˆ›å»ºè¿™ä¸ªç›®å½•
    config_name = "qwen_lora_config"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return False
    
    if not os.path.exists(dataset_path):
        print(f"âŒ æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {dataset_path}")
        return False
    
    # åˆ›å»ºLoRAé…ç½®ç›®å½•å’Œæ–‡ä»¶
    os.makedirs(lora_config_dir, exist_ok=True)
    lora_config_file = os.path.join(lora_config_dir, f"{config_name}.json")
    
    if not os.path.exists(lora_config_file):
        # åˆ›å»ºé»˜è®¤LoRAé…ç½®
        default_config = {
            "r": 8,
            "lora_alpha": 16,
            "lora_dropout": 0.05,
            "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"],
            "bias": "none"
        }
        with open(lora_config_file, 'w') as f:
            json.dump(default_config, f, indent=2)
        print(f"âœ… åˆ›å»ºé»˜è®¤LoRAé…ç½®: {lora_config_file}")
    
    try:
        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        print("ğŸ“¥ åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=torch.float16,
            load_in_8bit=True,
            device_map="auto"
        )
        
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        tokenizer.pad_token = tokenizer.eos_token
        
        print("âœ… æ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½æˆåŠŸ")
        
        # æµ‹è¯•è®­ç»ƒæ¨¡å—
        print("\nğŸ”§ æµ‹è¯•è®­ç»ƒæ¨¡å—...")
        try:
            train_result = train_model_on_dataset_with_progress(
                model=model,
                tokenizer=tokenizer,
                train_dataset_path=dataset_path,
                lora_config_dir=lora_config_dir,
                config_name=config_name,
                dataset_index=1,
                total_datasets=1
            )
            
            print("âœ… è®­ç»ƒæ¨¡å—æµ‹è¯•æˆåŠŸ")
            print(f"   è®­ç»ƒæ—¶é—´: {train_result['training_time']:.2f}ç§’")
            print(f"   è®­ç»ƒæ ·æœ¬æ•°: {train_result['train_samples']}")
            print(f"   æœ€ç»ˆloss: {train_result['final_loss']:.4f}")
            
        except Exception as e:
            print(f"âŒ è®­ç»ƒæ¨¡å—æµ‹è¯•å¤±è´¥: {e}")
            return False
        
        # æµ‹è¯•è¯„ä¼°æ¨¡å—
        print("\nğŸ” æµ‹è¯•è¯„ä¼°æ¨¡å—...")
        try:
            eval_result = evaluate_model_on_dataset_with_progress(
                model=model,
                tokenizer=tokenizer,
                eval_dataset_path=dataset_path,
                dataset_index=1,
                total_datasets=1
            )
            
            print("âœ… è¯„ä¼°æ¨¡å—æµ‹è¯•æˆåŠŸ")
            print(f"   è¯„ä¼°æ—¶é—´: {eval_result['evaluation_time']:.2f}ç§’")
            print(f"   è¯„ä¼°æ ·æœ¬æ•°: {eval_result['eval_samples']}")
            print(f"   EM Score: {eval_result['metrics']['exact_match']:.4f}")
            print(f"   F1 Score: {eval_result['metrics']['f1']:.4f}")
            
        except Exception as e:
            print(f"âŒ è¯„ä¼°æ¨¡å—æµ‹è¯•å¤±è´¥: {e}")
            return False
        
        # æ¸…ç†èµ„æº
        print("\nğŸ§¹ æ¸…ç†èµ„æº...")
        del model, tokenizer
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        print("âœ… å•ä¸ªæ¨¡å—æµ‹è¯•å®Œæˆ")
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å—æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return False

def test_full_pipeline():
    """æµ‹è¯•å®Œæ•´æµæ°´çº¿"""
    print("\n" + "=" * 80)
    print("ğŸš€ æµ‹è¯•å®Œæ•´æµæ°´çº¿")
    print("=" * 80)
    
    # æµ‹è¯•é…ç½®
    dataset_paths = [
        "/home/haonan/data_decision/example/qwen_dataset/qa_dataset_20250709_1_25.json"
    ]
    
    model_path = "/home/haonan/data_decision/models/Qwen/Qwen2.5-7B-Instruct-1M"
    lora_config_dir = "/home/haonan/data_decision/lora_configs"
    config_name = "qwen_lora_config"
    result_output_path = "/home/haonan/data_decision/results/test_pipeline_results.json"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return False
    
    for dataset_path in dataset_paths:
        if not os.path.exists(dataset_path):
            print(f"âŒ æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {dataset_path}")
            return False
    
    try:
        # è¿è¡Œå®Œæ•´æµæ°´çº¿
        print("ğŸ”„ å¼€å§‹è¿è¡Œå®Œæ•´æµæ°´çº¿...")
        start_time = time.time()
        
        summary = run_pipeline_with_progress(
            dataset_paths=dataset_paths,
            model_path=model_path,
            lora_config_dir=lora_config_dir,
            config_name=config_name,
            result_output_path=result_output_path
        )
        
        pipeline_time = time.time() - start_time
        
        # éªŒè¯ç»“æœ
        print("\nğŸ“Š éªŒè¯æµæ°´çº¿ç»“æœ...")
        
        # æ£€æŸ¥ç»“æœæ–‡ä»¶
        if os.path.exists(result_output_path):
            print(f"âœ… ç»“æœæ–‡ä»¶å·²ä¿å­˜: {result_output_path}")
            
            # è¯»å–å¹¶æ˜¾ç¤ºç»“æœæ‘˜è¦
            with open(result_output_path, 'r', encoding='utf-8') as f:
                results = json.load(f)
            
            pipeline_info = results['pipeline_info']
            summary_stats = results['summary_statistics']
            
            print(f"   æ€»æ‰§è¡Œæ—¶é—´: {pipeline_info['pipeline_time']:.2f}ç§’")
            print(f"   æˆåŠŸæ•°æ®é›†: {pipeline_info['successful_datasets']}/{pipeline_info['total_datasets']}")
            print(f"   å¤±è´¥æ•°æ®é›†: {pipeline_info['failed_datasets']}")
            
            if pipeline_info['successful_datasets'] > 0:
                print(f"   å¹³å‡EMåˆ†æ•°: {summary_stats['avg_em_score']:.4f}")
                print(f"   å¹³å‡F1åˆ†æ•°: {summary_stats['avg_f1_score']:.4f}")
                print(f"   æœ€ä½³EMåˆ†æ•°: {summary_stats['best_em_score']:.4f}")
                print(f"   æœ€ä½³F1åˆ†æ•°: {summary_stats['best_f1_score']:.4f}")
            
            # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
            print("\nğŸ“‹ è¯¦ç»†ç»“æœ:")
            for i, dataset_result in enumerate(results['dataset_results'], 1):
                if 'error' not in dataset_result:
                    training = dataset_result['training']
                    evaluation = dataset_result['evaluation']
                    print(f"   æ•°æ®é›† {i}: {dataset_result['dataset_name']}")
                    print(f"     è®­ç»ƒæ—¶é—´: {training['training_time']:.2f}ç§’")
                    print(f"     è®­ç»ƒæ ·æœ¬: {training['train_samples']}")
                    print(f"     æœ€ç»ˆloss: {training['final_loss']:.4f}")
                    print(f"     è¯„ä¼°æ—¶é—´: {evaluation['evaluation_time']:.2f}ç§’")
                    print(f"     è¯„ä¼°æ ·æœ¬: {evaluation['eval_samples']}")
                    print(f"     EM Score: {evaluation['metrics']['exact_match']:.4f}")
                    print(f"     F1 Score: {evaluation['metrics']['f1']:.4f}")
                else:
                    print(f"   æ•°æ®é›† {i}: {dataset_result['dataset_name']} - é”™è¯¯: {dataset_result['error']}")
        
        else:
            print(f"âŒ ç»“æœæ–‡ä»¶æœªç”Ÿæˆ: {result_output_path}")
            return False
        
        print(f"\nâœ… å®Œæ•´æµæ°´çº¿æµ‹è¯•æˆåŠŸï¼")
        print(f"   å®é™…æ‰§è¡Œæ—¶é—´: {pipeline_time:.2f}ç§’")
        return True
        
    except Exception as e:
        print(f"âŒ å®Œæ•´æµæ°´çº¿æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_memory_management():
    """æµ‹è¯•å†…å­˜ç®¡ç†"""
    print("\n" + "=" * 80)
    print("ğŸ§  æµ‹è¯•å†…å­˜ç®¡ç†")
    print("=" * 80)
    
    if torch.cuda.is_available():
        print("ğŸ” æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨æƒ…å†µ...")
        
        # è®°å½•åˆå§‹å†…å­˜
        initial_memory = torch.cuda.memory_allocated() / 1024**3
        print(f"   åˆå§‹GPUå†…å­˜ä½¿ç”¨: {initial_memory:.2f} GB")
        
        # è¿è¡Œä¸€ä¸ªç®€å•çš„æµ‹è¯•
        try:
            from transformers import AutoModelForCausalLM, AutoTokenizer
            
            model_path = "/home/haonan/data_decision/models/Qwen/Qwen2.5-7B-Instruct-1M"
            
            if os.path.exists(model_path):
                print("ğŸ“¥ åŠ è½½æ¨¡å‹æµ‹è¯•å†…å­˜...")
                
                model = AutoModelForCausalLM.from_pretrained(
                    model_path,
                    trust_remote_code=True,
                    torch_dtype=torch.float16,
                    load_in_8bit=True,
                    device_map="auto"
                )
                
                tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
                tokenizer.pad_token = tokenizer.eos_token
                
                # è®°å½•åŠ è½½åå†…å­˜
                loaded_memory = torch.cuda.memory_allocated() / 1024**3
                print(f"   åŠ è½½åGPUå†…å­˜ä½¿ç”¨: {loaded_memory:.2f} GB")
                print(f"   å†…å­˜å¢é•¿: {loaded_memory - initial_memory:.2f} GB")
                
                # æ¸…ç†èµ„æº
                print("ğŸ§¹ æ¸…ç†èµ„æº...")
                del model, tokenizer
                gc.collect()
                torch.cuda.empty_cache()
                
                # è®°å½•æ¸…ç†åå†…å­˜
                final_memory = torch.cuda.memory_allocated() / 1024**3
                print(f"   æ¸…ç†åGPUå†…å­˜ä½¿ç”¨: {final_memory:.2f} GB")
                print(f"   å†…å­˜é‡Šæ”¾: {loaded_memory - final_memory:.2f} GB")
                
                if final_memory <= initial_memory + 0.1:  # å…è®¸0.1GBçš„è¯¯å·®
                    print("âœ… å†…å­˜ç®¡ç†æµ‹è¯•é€šè¿‡")
                    return True
                else:
                    print("âš ï¸  å†…å­˜å¯èƒ½æœªå®Œå…¨é‡Šæ”¾")
                    return False
            else:
                print("âš ï¸  æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ï¼Œè·³è¿‡å†…å­˜æµ‹è¯•")
                return True
        except Exception as e:
            print(f"âŒ å†…å­˜ç®¡ç†æµ‹è¯•å¤±è´¥: {e}")
            return False
    else:
        print("âš ï¸  æœªæ£€æµ‹åˆ°GPUï¼Œè·³è¿‡å†…å­˜æµ‹è¯•")
        return True

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¯ å¼€å§‹å®Œæ•´æµç¨‹æµ‹è¯•")
    print("=" * 80)
    
    # åˆ›å»ºç»“æœç›®å½•
    results_dir = "/home/haonan/data_decision/results"
    os.makedirs(results_dir, exist_ok=True)
    
    # æµ‹è¯•ç»“æœ
    test_results = {
        "individual_modules": False,
        "full_pipeline": False,
        "memory_management": False
    }
    
    # 1. æµ‹è¯•å•ä¸ªæ¨¡å—
    print("\nğŸ“‹ æµ‹è¯•é¡¹ç›®:")
    print("1. å•ä¸ªæ¨¡å—åŠŸèƒ½æµ‹è¯•")
    print("2. å®Œæ•´æµæ°´çº¿æµ‹è¯•")
    print("3. å†…å­˜ç®¡ç†æµ‹è¯•")
    
    test_results["individual_modules"] = test_individual_modules()
    
    # 2. æµ‹è¯•å®Œæ•´æµæ°´çº¿
    test_results["full_pipeline"] = test_full_pipeline()
    
    # 3. æµ‹è¯•å†…å­˜ç®¡ç†
    test_results["memory_management"] = test_memory_management()
    
    # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
    print("\n" + "=" * 80)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 80)
    
    for test_name, result in test_results.items():
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{test_name}: {status}")
    
    all_passed = all(test_results.values())
    
    if all_passed:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿå¯ä»¥æ­£å¸¸ä½¿ç”¨ã€‚")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³é…ç½®ã€‚")
    
    print("=" * 80)
    
    return all_passed

if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\nâ¹ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°æœªé¢„æœŸçš„é”™è¯¯: {e}")
        sys.exit(1) 