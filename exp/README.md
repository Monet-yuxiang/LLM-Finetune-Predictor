# ğŸ¤– å¤§æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°æµæ°´çº¿

> åŸºäº LoRA çš„é«˜æ•ˆå¤§æ¨¡å‹å¾®è°ƒä¸è¯„ä¼°ç³»ç»Ÿï¼Œä¸“ä¸º RTX 4090 å¤šå¡ç¯å¢ƒä¼˜åŒ–

[![Python](https://img.shields.io/badge/Python-3.11-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.1.0-red.svg)](https://pytorch.org/)
[![CUDA](https://img.shields.io/badge/CUDA-11.8-green.svg)](https://developer.nvidia.com/cuda-toolkit)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## ğŸ“‹ ç›®å½•

- [é¡¹ç›®æ¦‚è¿°](#é¡¹ç›®æ¦‚è¿°)
- [æ ¸å¿ƒç‰¹æ€§](#æ ¸å¿ƒç‰¹æ€§)
- [ç³»ç»Ÿè¦æ±‚](#ç³»ç»Ÿè¦æ±‚)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [æ¨¡å—è¯´æ˜](#æ¨¡å—è¯´æ˜)
- [ä½¿ç”¨æŒ‡å—](#ä½¿ç”¨æŒ‡å—)
- [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)
- [è´¡çŒ®æŒ‡å—](#è´¡çŒ®æŒ‡å—)

## ğŸ¯ é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªä¸“ä¸º RTX 4090 å¤šå¡ç¯å¢ƒè®¾è®¡çš„å¤§æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°æµæ°´çº¿ç³»ç»Ÿã€‚é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œæ”¯æŒ LoRA å¾®è°ƒã€å®æ—¶è¿›åº¦ç›‘æ§ã€å†…å­˜ä¼˜åŒ–ç®¡ç†ï¼Œé€‚ç”¨äºå¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ä»»åŠ¡ã€‚

### ğŸ—ï¸ æ¶æ„è®¾è®¡

```
training/
â”œâ”€â”€ train_module.py          # è®­ç»ƒæ¨¡å—
â”œâ”€â”€ evaluate_module.py       # è¯„ä¼°æ¨¡å—  
â”œâ”€â”€ pipeline_module.py       # æµæ°´çº¿æ¨¡å—
â”œâ”€â”€ test_pipeline.py         # æµ‹è¯•è„šæœ¬
â”œâ”€â”€ qwen_gpu_env.yaml       # ç¯å¢ƒé…ç½®
â”œâ”€â”€ install_qwen_gpu_env.sh # ç¯å¢ƒå®‰è£…è„šæœ¬
â””â”€â”€ download_qwen.py        # æ¨¡å‹ä¸‹è½½è„šæœ¬
```

## âœ¨ æ ¸å¿ƒç‰¹æ€§

### ğŸš€ é«˜æ•ˆè®­ç»ƒ
- **LoRA å¾®è°ƒ**ï¼šå‚æ•°é«˜æ•ˆå¾®è°ƒï¼Œå¤§å¹…å‡å°‘æ˜¾å­˜å ç”¨
- **å¤šå¡æ”¯æŒ**ï¼šè‡ªåŠ¨é€‚é… RTX 4090 å¤šå¡ç¯å¢ƒ
- **å†…å­˜ä¼˜åŒ–**ï¼šæ™ºèƒ½å†…å­˜ç®¡ç†ï¼Œé¿å… OOM é”™è¯¯
- **å®æ—¶ç›‘æ§**ï¼šè®­ç»ƒè¿›åº¦å®æ—¶æ˜¾ç¤ºï¼Œæ”¯æŒæ–­ç‚¹ç»­è®­

### ğŸ“Š ç²¾ç¡®è¯„ä¼°
- **å¤šæŒ‡æ ‡è¯„ä¼°**ï¼šæ”¯æŒ EM (Exact Match) å’Œ F1 åˆ†æ•°
- **æ‰¹é‡æ¨ç†**ï¼šæ™ºèƒ½æ‰¹æ¬¡å¤§å°æ¨èï¼Œä¼˜åŒ–æ¨ç†é€Ÿåº¦
- **æ•°æ®å…¼å®¹**ï¼šæ”¯æŒ JSONã€JSONLã€HuggingFace æ•°æ®é›†æ ¼å¼
- **ç»“æœå¯è§†åŒ–**ï¼šè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Šå’Œç»Ÿè®¡åˆ†æ

### ğŸ”„ æµæ°´çº¿ç®¡ç†
- **æ¨¡å—åŒ–è®¾è®¡**ï¼šè®­ç»ƒã€è¯„ä¼°ã€æµæ°´çº¿ç‹¬ç«‹æ¨¡å—
- **èµ„æºå¤ç”¨**ï¼šæ¯è½®é‡æ–°åŠ è½½åŸå§‹æ¨¡å‹ï¼Œé¿å…å†…å­˜ç´¯ç§¯
- **é”™è¯¯æ¢å¤**ï¼šå¼‚å¸¸å¤„ç†å’Œè‡ªåŠ¨èµ„æºæ¸…ç†
- **ç»“æœæ±‡æ€»**ï¼šè‡ªåŠ¨ç”Ÿæˆ JSON æ ¼å¼çš„è¯¦ç»†æŠ¥å‘Š

## ğŸ’» ç³»ç»Ÿè¦æ±‚

### ç¡¬ä»¶è¦æ±‚
- **GPU**: NVIDIA RTX 4090 (æ¨èå¤šå¡)
- **æ˜¾å­˜**: æ¯å¡ 24GB+ 
- **å†…å­˜**: 64GB+ RAM
- **å­˜å‚¨**: 100GB+ å¯ç”¨ç©ºé—´

### è½¯ä»¶è¦æ±‚
- **æ“ä½œç³»ç»Ÿ**: Linux (Ubuntu 20.04+)
- **Python**: 3.11
- **CUDA**: 11.8+
- **é©±åŠ¨**: NVIDIA Driver 535+

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå®‰è£…

```bash
# å…‹éš†é¡¹ç›®
cd /home/haonan/data_decision/training

# ä¸€é”®å®‰è£…ç¯å¢ƒ
bash install_qwen_gpu_env.sh

# æ¿€æ´»ç¯å¢ƒ
conda activate qwen_gpu_env
```

### 2. æ¨¡å‹ä¸‹è½½

```bash
# ä¸‹è½½ Qwen2.5-7B-Instruct-1M æ¨¡å‹
python download_qwen.py
```

### 3. è¿è¡Œæµ‹è¯•

```bash
# æµ‹è¯•å®Œæ•´æµæ°´çº¿
python test_pipeline.py
```

## ğŸ“¦ æ¨¡å—è¯´æ˜

### ğŸ¯ train_module.py - è®­ç»ƒæ¨¡å—

**ä¸»è¦åŠŸèƒ½**ï¼š
- LoRA é…ç½®åŠ è½½å’Œç®¡ç†
- æ•°æ®é›†é¢„å¤„ç†å’Œæ ¼å¼åŒ–
- æ¨¡å‹å¾®è°ƒè®­ç»ƒ
- è®­ç»ƒç»“æœç»Ÿè®¡

**æ ¸å¿ƒå‡½æ•°**ï¼š
```python
def train_model_on_dataset_with_progress(
    model, tokenizer, train_dataset_path, 
    lora_config_dir, config_name, 
    dataset_index=1, total_datasets=1, 
    training_args=None
) -> Dict[str, Any]
```

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```python
from train_module import train_model_on_dataset_with_progress

# è®­ç»ƒæ¨¡å‹
train_result = train_model_on_dataset_with_progress(
    model=model,
    tokenizer=tokenizer,
    train_dataset_path="path/to/dataset.json",
    lora_config_dir="path/to/lora_configs",
    config_name="qwen_lora_config"
)
```

### ğŸ” evaluate_module.py - è¯„ä¼°æ¨¡å—

**ä¸»è¦åŠŸèƒ½**ï¼š
- å¤šæ ¼å¼æ•°æ®é›†åŠ è½½
- æ‰¹é‡æ¨ç†ç”Ÿæˆ
- EM/F1 æŒ‡æ ‡è®¡ç®—
- å†…å­˜ä¼˜åŒ–æ¨ç†

**æ ¸å¿ƒå‡½æ•°**ï¼š
```python
def evaluate_model_on_dataset_with_progress(
    model, tokenizer, eval_dataset_path,
    dataset_index=1, total_datasets=1,
    batch_size=None, generation_kwargs=None
) -> Dict[str, Any]
```

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```python
from evaluate_module import evaluate_model_on_dataset_with_progress

# è¯„ä¼°æ¨¡å‹
eval_result = evaluate_model_on_dataset_with_progress(
    model=model,
    tokenizer=tokenizer,
    eval_dataset_path="path/to/eval_dataset.json",
    batch_size=4
)
```

### ğŸ”„ pipeline_module.py - æµæ°´çº¿æ¨¡å—

**ä¸»è¦åŠŸèƒ½**ï¼š
- è®­ç»ƒ-è¯„ä¼°æµæ°´çº¿ç®¡ç†
- å¤šæ•°æ®é›†å¾ªç¯å¤„ç†
- èµ„æºç®¡ç†å’Œæ¸…ç†
- ç»“æœæ±‡æ€»å’Œä¿å­˜

**æ ¸å¿ƒå‡½æ•°**ï¼š
```python
def run_training_evaluation_pipeline(
    dataset_paths, model_path, lora_config_dir,
    config_name, result_output_path,
    training_args=None, evaluation_batch_size=None,
    evaluation_generation_kwargs=None
) -> Dict[str, Any]
```

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```python
from pipeline_module import run_pipeline_with_progress

# è¿è¡Œå®Œæ•´æµæ°´çº¿
summary = run_pipeline_with_progress(
    dataset_paths=["dataset1.json", "dataset2.json"],
    model_path="/path/to/model",
    lora_config_dir="/path/to/lora_configs",
    config_name="qwen_lora_config",
    result_output_path="/path/to/results.json"
)
```

## ğŸ“– ä½¿ç”¨æŒ‡å—

### æ•°æ®æ ¼å¼è¦æ±‚

#### è®­ç»ƒæ•°æ®æ ¼å¼
```json
[
  {
    "messages": [
      {
        "role": "user",
        "content": "Context: åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½ã€‚\nQuestion: ä¸­å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œï¼Ÿ"
      },
      {
        "role": "assistant", 
        "content": "åŒ—äº¬"
      }
    ]
  }
]
```

#### è¯„ä¼°æ•°æ®æ ¼å¼
```json
[
  {
    "messages": [
      {
        "role": "user",
        "content": "Context: åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½ã€‚\nQuestion: ä¸­å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œï¼Ÿ"
      },
      {
        "role": "assistant",
        "content": "åŒ—äº¬"
      }
    ]
  }
]
```

### LoRA é…ç½®

åˆ›å»º `lora_configs/qwen_lora_config.json`ï¼š
```json
{
  "r": 8,
  "lora_alpha": 16,
  "lora_dropout": 0.05,
  "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"],
  "bias": "none"
}
```

### è®­ç»ƒå‚æ•°

```python
training_args = {
    "num_train_epochs": 3,
    "per_device_train_batch_size": 8,
    "gradient_accumulation_steps": 8,
    "learning_rate": 1e-4,
    "max_grad_norm": 0.3,
    "warmup_ratio": 0.03
}
```

## âš™ï¸ é…ç½®è¯´æ˜

### ç¯å¢ƒé…ç½® (qwen_gpu_env.yaml)

```yaml
name: qwen_gpu_env
channels:
  - defaults
  - conda-forge
  - pytorch
  - nvidia
dependencies:
  - python=3.11
  - pytorch=2.1.0
  - pytorch-cuda=11.8
  - transformers==4.38.2
  - peft==0.7.1
  - bitsandbytes==0.43.0
  - accelerate==0.26.1
  - datasets==2.13.0
  - evaluate==0.4.1
```

### ç”Ÿæˆå‚æ•°é…ç½®

```python
generation_kwargs = {
    'max_new_tokens': 40,
    'do_sample': False,  # è´ªå©ªè§£ç 
    'temperature': 1.0,
    'repetition_penalty': 1.1,
    'num_beams': 1
}
```

## ğŸš€ æ€§èƒ½ä¼˜åŒ–

### å†…å­˜ä¼˜åŒ–ç­–ç•¥

1. **æ¨¡å‹é‡åŒ–**ï¼šä½¿ç”¨ 8-bit é‡åŒ–å‡å°‘æ˜¾å­˜å ç”¨
2. **æ‰¹æ¬¡ä¼˜åŒ–**ï¼šæ™ºèƒ½æ‰¹æ¬¡å¤§å°æ¨è
3. **èµ„æºæ¸…ç†**ï¼šè®­ç»ƒåè‡ªåŠ¨æ¸…ç† LoRA æƒé‡
4. **å¤šè½®å¤ç”¨**ï¼šæ¯è½®é‡æ–°åŠ è½½åŸå§‹æ¨¡å‹

### GPU åˆ©ç”¨ç‡ä¼˜åŒ–

```bash
# ç›‘æ§ GPU ä½¿ç”¨æƒ…å†µ
watch -n 1 nvidia-smi

# æ£€æŸ¥å†…å­˜ä½¿ç”¨
python -c "import torch; print(f'GPU Memory: {torch.cuda.memory_allocated()/1024**3:.2f} GB')"
```

### å¤šå¡é…ç½®

```python
# è‡ªåŠ¨è®¾å¤‡æ˜ å°„
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="auto",  # è‡ªåŠ¨åˆ†é…åˆ°å¤šå¡
    torch_dtype=torch.float16,
    load_in_8bit=True
)
```

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. CUDA å†…å­˜ä¸è¶³
```bash
# è§£å†³æ–¹æ¡ˆï¼šå‡å°‘æ‰¹æ¬¡å¤§å°
batch_size = 4  # ä»é»˜è®¤å€¼å‡å°‘
```

#### 2. æ¨¡å‹åŠ è½½å¤±è´¥
```bash
# æ£€æŸ¥æ¨¡å‹è·¯å¾„
ls -la /path/to/model/

# é‡æ–°ä¸‹è½½æ¨¡å‹
python download_qwen.py
```

#### 3. ä¾èµ–åŒ…ç‰ˆæœ¬å†²çª
```bash
# é‡æ–°åˆ›å»ºç¯å¢ƒ
conda env remove -n qwen_gpu_env
bash install_qwen_gpu_env.sh
```

#### 4. ç”Ÿæˆå‚æ•°é”™è¯¯
```python
# ä½¿ç”¨ç¨³å®šçš„ç”Ÿæˆå‚æ•°
generation_kwargs = {
    'do_sample': False,  # é¿å…æ¦‚ç‡é—®é¢˜
    'num_beams': 1,     # å•æŸæœç´¢
    'early_stopping': False  # é¿å…è­¦å‘Š
}
```

### æ—¥å¿—åˆ†æ

```bash
# æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
tail -f training.log

# æŸ¥çœ‹è¯„ä¼°æ—¥å¿—  
tail -f evaluation.log

# æŸ¥çœ‹æµæ°´çº¿æ—¥å¿—
tail -f pipeline.log
```

## ğŸ“Š æ€§èƒ½åŸºå‡†

### RTX 4090 å•å¡æ€§èƒ½

| æ¨¡å‹å¤§å° | æ‰¹æ¬¡å¤§å° | è®­ç»ƒé€Ÿåº¦ | æ˜¾å­˜å ç”¨ | è¯„ä¼°é€Ÿåº¦ |
|---------|---------|---------|---------|---------|
| 7B      | 8       | ~2.5 steps/s | ~18GB | ~100 samples/s |
| 14B     | 4       | ~1.2 steps/s | ~22GB | ~50 samples/s |

### å¤šå¡æ‰©å±•

| å¡æ•° | æ€»æ‰¹æ¬¡å¤§å° | è®­ç»ƒåŠ é€Ÿæ¯” | æ˜¾å­˜åˆ©ç”¨ç‡ |
|------|-----------|-----------|-----------|
| 1    | 8         | 1x        | 75%       |
| 2    | 16        | 1.8x      | 80%       |
| 4    | 32        | 3.2x      | 85%       |

## ğŸ¤ è´¡çŒ®æŒ‡å—

### å¼€å‘ç¯å¢ƒè®¾ç½®

```bash
# å…‹éš†é¡¹ç›®
git clone <repository_url>
cd training

# å®‰è£…å¼€å‘ä¾èµ–
pip install -e .
pip install black flake8 pytest

# ä»£ç æ ¼å¼åŒ–
black *.py

# è¿è¡Œæµ‹è¯•
pytest test_*.py
```

### æäº¤è§„èŒƒ

- **feat**: æ–°åŠŸèƒ½
- **fix**: é”™è¯¯ä¿®å¤
- **docs**: æ–‡æ¡£æ›´æ–°
- **style**: ä»£ç æ ¼å¼
- **refactor**: ä»£ç é‡æ„
- **test**: æµ‹è¯•ç›¸å…³
- **chore**: æ„å»ºè¿‡ç¨‹æˆ–è¾…åŠ©å·¥å…·çš„å˜åŠ¨

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚

## ğŸ™ è‡´è°¢

- [Hugging Face Transformers](https://github.com/huggingface/transformers)
- [PEFT](https://github.com/huggingface/peft)
- [Qwen Team](https://github.com/QwenLM/Qwen)
- [BitsAndBytes](https://github.com/TimDettmers/bitsandbytes)

---

**â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ªæ˜Ÿæ ‡ï¼**

---

*æœ€åæ›´æ–°ï¼š2024å¹´7æœˆ26æ—¥* 