"""
æ€»åŒ…æ–‡ä»¶ - æ•´åˆè®­ç»ƒå’Œè¯„ä¼°æ¨¡å—
ä¸»è¦åŠŸèƒ½ï¼š
1. æ•°æ®é›†å¾ªç¯ï¼šå¯¹æ¯ä¸ªæ•°æ®é›†è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°
2. æ¨¡å‹å¤ç”¨ï¼šç¡®ä¿åŒä¸€æ¨¡å‹å®ä¾‹ç”¨äºè®­ç»ƒå’Œè¯„ä¼°
3. å†…å­˜ç®¡ç†ï¼šå¾ªç¯ç»“æŸåå½»åº•é‡Šæ”¾æ‰€æœ‰èµ„æº
4. è¿›åº¦æ˜¾ç¤ºï¼šå®æ—¶æ˜¾ç¤ºå½“å‰å¾ªç¯è¿›åº¦å’ŒçŠ¶æ€
5. ç»“æœæ±‡æ€»ï¼šå°†æ‰€æœ‰å…³é”®ä¿¡æ¯å†™å…¥JSONæ–‡ä»¶
"""

import os
import json
import time
import gc
import torch
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional
from tqdm import tqdm

# å¯¼å…¥è®­ç»ƒå’Œè¯„ä¼°æ¨¡å—
from train_module import train_model_on_dataset_with_progress
from evaluate_module import evaluate_model_on_dataset_with_progress

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def setup_logging():
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('pipeline.log'),
            logging.StreamHandler()
        ]
    )

def load_model_and_tokenizer(model_path: str):
    """
    åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
        
    Returns:
        model, tokenizer: æ¨¡å‹å’Œåˆ†è¯å™¨å®ä¾‹
    """
    try:
        logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
        
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        # åŠ è½½æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=torch.float16,
            load_in_8bit=True,
            device_map="auto"
        )
        
        # åŠ è½½åˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        tokenizer.pad_token = tokenizer.eos_token
        
        logger.info("æ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½æˆåŠŸ")
        return model, tokenizer
        
    except Exception as e:
        logger.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        raise

def run_training_evaluation_pipeline(
    dataset_paths: List[str],
    model_path: str,
    lora_config_dir: str,
    config_name: str,
    result_output_path: str,
    training_args: Optional[Dict[str, Any]] = None,
    evaluation_batch_size: Optional[int] = None,
    evaluation_generation_kwargs: Optional[Dict] = None
) -> Dict[str, Any]:
    """
    è¿è¡Œè®­ç»ƒ-è¯„ä¼°æµæ°´çº¿
    
    Args:
        dataset_paths: æ•°æ®é›†è·¯å¾„åˆ—è¡¨
        model_path: æ¨¡å‹è·¯å¾„
        lora_config_dir: LoRAé…ç½®æ–‡ä»¶ç›®å½•è·¯å¾„
        config_name: é…ç½®æ–‡ä»¶å
        result_output_path: ç»“æœè¾“å‡ºæ–‡ä»¶è·¯å¾„
        training_args: è®­ç»ƒå‚æ•°ï¼ˆå¯é€‰ï¼‰
        evaluation_batch_size: è¯„ä¼°æ‰¹æ¬¡å¤§å°ï¼ˆå¯é€‰ï¼‰
        evaluation_generation_kwargs: è¯„ä¼°ç”Ÿæˆå‚æ•°ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        æµæ°´çº¿æ‰§è¡Œæ‘˜è¦
    """
    start_time = time.time()
    pipeline_results = []
    
    try:
        # 1. è®¾ç½®æ—¥å¿—
        setup_logging()
        logger.info("=" * 80)
        logger.info("å¼€å§‹è®­ç»ƒ-è¯„ä¼°æµæ°´çº¿")
        logger.info("=" * 80)
        logger.info(f"æ¨¡å‹è·¯å¾„: {model_path}")
        logger.info(f"LoRAé…ç½®ç›®å½•: {lora_config_dir}")
        logger.info(f"LoRAé…ç½®åç§°: {config_name}")
        logger.info(f"æ•°æ®é›†æ•°é‡: {len(dataset_paths)}")
        logger.info(f"ç»“æœè¾“å‡ºè·¯å¾„: {result_output_path}")
        
        # 2. å¼€å§‹æ•°æ®é›†å¾ªç¯
        total_datasets = len(dataset_paths)
        
        for dataset_index, dataset_path in enumerate(dataset_paths, 1):
            dataset_name = os.path.basename(dataset_path)
            
            logger.info(f"\n{'='*80}")
            logger.info(f"å¤„ç†æ•°æ®é›† {dataset_index}/{total_datasets}: {dataset_name}")
            logger.info(f"{'='*80}")
            
            try:
                # æ¯è½®éƒ½é‡æ–°åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ï¼ˆç¡®ä¿ä½¿ç”¨åŸå§‹æ¨¡å‹ï¼‰
                logger.info("æ­£åœ¨åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
                model, tokenizer = load_model_and_tokenizer(model_path)
                
                # 3.1 è®­ç»ƒé˜¶æ®µ
                logger.info(f"\n--- å¼€å§‹è®­ç»ƒé˜¶æ®µ ---")
                train_summary = train_model_on_dataset_with_progress(
                    model=model,
                    tokenizer=tokenizer,
                    train_dataset_path=dataset_path,
                    lora_config_dir=lora_config_dir,
                    config_name=config_name,
                    dataset_index=dataset_index,
                    total_datasets=total_datasets,
                    training_args=training_args
                )
                
                # 3.2 è¯„ä¼°é˜¶æ®µ
                logger.info(f"\n--- å¼€å§‹è¯„ä¼°é˜¶æ®µ ---")
                eval_summary = evaluate_model_on_dataset_with_progress(
                    model=model,
                    tokenizer=tokenizer,
                    eval_dataset_path=dataset_path,
                    dataset_index=dataset_index,
                    total_datasets=total_datasets,
                    batch_size=evaluation_batch_size,
                    generation_kwargs=evaluation_generation_kwargs
                )
                
                # 3.3 åˆå¹¶ç»“æœ
                dataset_result = {
                    "dataset_index": dataset_index,
                    "dataset_name": dataset_name,
                    "dataset_path": dataset_path,
                    "training": train_summary,
                    "evaluation": eval_summary,
                    "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }
                
                pipeline_results.append(dataset_result)
                
                # 3.4 æ˜¾ç¤ºå½“å‰æ•°æ®é›†ç»“æœæ‘˜è¦
                logger.info(f"\n--- æ•°æ®é›† {dataset_index}/{total_datasets} å®Œæˆ ---")
                logger.info(f"è®­ç»ƒæ—¶é—´: {train_summary['training_time']:.2f}ç§’")
                logger.info(f"è®­ç»ƒæ ·æœ¬æ•°: {train_summary['train_samples']}")
                logger.info(f"åˆå§‹loss: {train_summary.get('initial_loss', 'N/A')}")
                logger.info(f"æœ€ç»ˆloss: {train_summary['final_loss']:.4f}")
                logger.info(f"è¯„ä¼°æ—¶é—´: {eval_summary['evaluation_time']:.2f}ç§’")
                logger.info(f"è¯„ä¼°æ ·æœ¬æ•°: {eval_summary['eval_samples']}")
                logger.info(f"EM Score: {eval_summary['metrics']['exact_match']:.4f}")
                logger.info(f"F1 Score: {eval_summary['metrics']['f1']:.4f}")
                
            except Exception as e:
                logger.error(f"å¤„ç†æ•°æ®é›† {dataset_name} æ—¶å‡ºé”™: {e}")
                # è®°å½•é”™è¯¯ä½†ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ•°æ®é›†
                error_result = {
                    "dataset_index": dataset_index,
                    "dataset_name": dataset_name,
                    "dataset_path": dataset_path,
                    "error": str(e),
                    "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }
                pipeline_results.append(error_result)
                continue
            
            # 3.5 æ¸…ç†å½“å‰è½®æ¬¡çš„æ‰€æœ‰èµ„æºï¼ˆåŒ…æ‹¬æ¨¡å‹ï¼‰
            logger.info("æ¸…ç†å½“å‰è½®æ¬¡èµ„æº...")
            
            # æ›´å½»åº•çš„å†…å­˜æ¸…ç†
            if torch.cuda.is_available():
                logger.info(f"æ¸…ç†å‰GPUå†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
            
            # æ¸…ç†æ¨¡å‹å’Œåˆ†è¯å™¨
            if 'model' in locals():
                del model
            if 'tokenizer' in locals():
                del tokenizer
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                logger.info(f"æ¸…ç†åGPUå†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
        
        # 4. ç”Ÿæˆæœ€ç»ˆç»“æœæ‘˜è¦
        pipeline_time = time.time() - start_time
        successful_datasets = [r for r in pipeline_results if 'error' not in r]
        failed_datasets = [r for r in pipeline_results if 'error' in r]
        
        final_summary = {
            "pipeline_info": {
                "model_path": model_path,
                "lora_config_dir": lora_config_dir,
                "config_name": config_name,
                "total_datasets": total_datasets,
                "successful_datasets": len(successful_datasets),
                "failed_datasets": len(failed_datasets),
                "pipeline_time": pipeline_time,
                "start_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            },
            "dataset_results": pipeline_results,
            "summary_statistics": {
                "avg_training_time": sum(r['training']['training_time'] for r in successful_datasets) / len(successful_datasets) if successful_datasets else 0,
                "avg_evaluation_time": sum(r['evaluation']['evaluation_time'] for r in successful_datasets) / len(successful_datasets) if successful_datasets else 0,
                "avg_em_score": sum(r['evaluation']['metrics']['exact_match'] for r in successful_datasets) / len(successful_datasets) if successful_datasets else 0,
                "avg_f1_score": sum(r['evaluation']['metrics']['f1'] for r in successful_datasets) / len(successful_datasets) if successful_datasets else 0,
                "best_em_score": max(r['evaluation']['metrics']['exact_match'] for r in successful_datasets) if successful_datasets else 0,
                "best_f1_score": max(r['evaluation']['metrics']['f1'] for r in successful_datasets) if successful_datasets else 0
            }
        }
        
        # 5. ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
        logger.info(f"\næ­£åœ¨ä¿å­˜ç»“æœåˆ°: {result_output_path}")
        os.makedirs(os.path.dirname(result_output_path), exist_ok=True)
        
        with open(result_output_path, 'w', encoding='utf-8') as f:
            json.dump(final_summary, f, indent=2, ensure_ascii=False)
        
        # 6. æ˜¾ç¤ºæœ€ç»ˆæ‘˜è¦
        logger.info("=" * 80)
        logger.info("æµæ°´çº¿æ‰§è¡Œå®Œæˆï¼")
        logger.info("=" * 80)
        logger.info(f"æ€»æ‰§è¡Œæ—¶é—´: {pipeline_time:.2f}ç§’")
        logger.info(f"æˆåŠŸå¤„ç†æ•°æ®é›†: {len(successful_datasets)}/{total_datasets}")
        logger.info(f"å¤±è´¥æ•°æ®é›†: {len(failed_datasets)}")
        if successful_datasets:
            logger.info(f"å¹³å‡EMåˆ†æ•°: {final_summary['summary_statistics']['avg_em_score']:.4f}")
            logger.info(f"å¹³å‡F1åˆ†æ•°: {final_summary['summary_statistics']['avg_f1_score']:.4f}")
            logger.info(f"æœ€ä½³EMåˆ†æ•°: {final_summary['summary_statistics']['best_em_score']:.4f}")
            logger.info(f"æœ€ä½³F1åˆ†æ•°: {final_summary['summary_statistics']['best_f1_score']:.4f}")
        logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {result_output_path}")
        logger.info("=" * 80)
        
        return final_summary
        
    except Exception as e:
        logger.error(f"æµæ°´çº¿æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        raise
        
    finally:
        # 7. å½»åº•æ¸…ç†æ‰€æœ‰èµ„æº
        logger.info("æ­£åœ¨æ¸…ç†æ‰€æœ‰èµ„æº...")
        
        # æ¸…ç†ç»“æœåˆ—è¡¨
        if 'pipeline_results' in locals():
            del pipeline_results
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        logger.info("æ‰€æœ‰èµ„æºæ¸…ç†å®Œæˆ")

def run_pipeline_with_progress(
    dataset_paths: List[str],
    model_path: str,
    lora_config_dir: str,
    config_name: str,
    result_output_path: str,
    training_args: Optional[Dict[str, Any]] = None,
    evaluation_batch_size: Optional[int] = None,
    evaluation_generation_kwargs: Optional[Dict] = None
) -> Dict[str, Any]:
    """
    å¸¦è¿›åº¦æ˜¾ç¤ºçš„æµæ°´çº¿æ‰§è¡Œå‡½æ•°
    
    Args:
        dataset_paths: æ•°æ®é›†è·¯å¾„åˆ—è¡¨
        model_path: æ¨¡å‹è·¯å¾„
        lora_config_dir: LoRAé…ç½®æ–‡ä»¶ç›®å½•è·¯å¾„
        config_name: é…ç½®æ–‡ä»¶å
        result_output_path: ç»“æœè¾“å‡ºæ–‡ä»¶è·¯å¾„
        training_args: è®­ç»ƒå‚æ•°ï¼ˆå¯é€‰ï¼‰
        evaluation_batch_size: è¯„ä¼°æ‰¹æ¬¡å¤§å°ï¼ˆå¯é€‰ï¼‰
        evaluation_generation_kwargs: è¯„ä¼°ç”Ÿæˆå‚æ•°ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        æµæ°´çº¿æ‰§è¡Œæ‘˜è¦
    """
    print(f"\n{'='*80}")
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ-è¯„ä¼°æµæ°´çº¿")
    print(f"{'='*80}")
    print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"âš™ï¸  LoRAé…ç½®: {config_name}")
    print(f"ğŸ“Š æ•°æ®é›†æ•°é‡: {len(dataset_paths)}")
    print(f"ğŸ’¾ ç»“æœè¾“å‡º: {result_output_path}")
    print(f"{'='*80}\n")
    
    return run_training_evaluation_pipeline(
        dataset_paths=dataset_paths,
        model_path=model_path,
        lora_config_dir=lora_config_dir,
        config_name=config_name,
        result_output_path=result_output_path,
        training_args=training_args,
        evaluation_batch_size=evaluation_batch_size,
        evaluation_generation_kwargs=evaluation_generation_kwargs
    )

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    setup_logging()
    
    # ç¤ºä¾‹ç”¨æ³•
    dataset_paths = [
        "/home/haonan/data_decision/example/qwen_dataset/qa_dataset_20250709_1_25.json",
        # å¯ä»¥æ·»åŠ æ›´å¤šæ•°æ®é›†è·¯å¾„
    ]
    
    model_path = "/home/haonan/data_decision/models/Qwen/Qwen2.5-7B-Instruct-1M"
    lora_config_dir = "/path/to/lora_configs"  # éœ€è¦è®¾ç½®å®é™…çš„LoRAé…ç½®ç›®å½•
    config_name = "qwen_lora_config"
    result_output_path = "/home/haonan/data_decision/results/pipeline_results.json"
    
    try:
        # è¿è¡Œæµæ°´çº¿
        summary = run_pipeline_with_progress(
            dataset_paths=dataset_paths,
            model_path=model_path,
            lora_config_dir=lora_config_dir,
            config_name=config_name,
            result_output_path=result_output_path
        )
        
        print(f"\nâœ… æµæ°´çº¿æ‰§è¡Œå®Œæˆï¼")
        print(f"ğŸ“Š ç»“æœæ‘˜è¦: {json.dumps(summary['pipeline_info'], indent=2, ensure_ascii=False)}")
        
    except Exception as e:
        logger.error(f"æµ‹è¯•å¤±è´¥: {e}")
        print(f"âŒ æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}") 